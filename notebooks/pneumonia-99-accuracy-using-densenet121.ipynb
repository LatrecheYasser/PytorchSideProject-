{
  "cells": [
    {
      "metadata": {
        "_uuid": "05522a0ae2dc9493e8480daf0f0ca915a8a62619"
      },
      "cell_type": "markdown",
      "source": "#                                                                                    Pneumonia \nis an inflammatory condition of the lung affecting primarily the small air sacs known as alveoli, Typically symptoms include some combination of productive or dry cough, chest pain, fever, and trouble breathing. Severity is variable.\n\nPneumonia is usually caused by infection with viruses or bacteria and less commonly by other microorganisms, certain medications and conditions such as autoimmune diseases.  Risk factors include other lung diseases such as cystic fibrosis, COPD, and asthma, diabetes, heart failure, a history of smoking, a poor ability to cough such as following a stroke, or a weak immune system. Diagnosis is often based on the symptoms and physical examination. Chest X-ray, blood tests, and culture of the sputum may help confirm the diagnosis. The disease may be classified by where it was acquired with community, hospital, or health care associated pneumonia. **[See the source](https://en.wikipedia.org/wiki/Pneumonia)**"
    },
    {
      "metadata": {
        "_uuid": "8009da3a05499970c3bbad14c9ef2b66c119ce90"
      },
      "cell_type": "markdown",
      "source": "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/2a/Chest_X-ray_in_influenza_and_Haemophilus_influenzae_-_annotated.jpg/300px-Chest_X-ray_in_influenza_and_Haemophilus_influenzae_-_annotated.jpg\"> \n"
    },
    {
      "metadata": {
        "_uuid": "03240cff2d96a8c484adad95fbd67b88e3120c6f"
      },
      "cell_type": "markdown",
      "source": "As Pneumonia is responsible for more than 1 million hospitalizations and 50,000 deaths per year in the US alone, we think that it's a must to use our knowledge to help those people. \nOf course there is a lot of reserach on this, and there is some Neural Networks ready to use but we tried to work on a new way so maybe we can build a better model :D"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d551b7bd885ed4fdd2cba8edbad404623db771e8"
      },
      "cell_type": "markdown",
      "source": "Speaking Neural Network now. We used a pretrained DenseNet121 with a different classifier of 4 layer (1024 -> 256 -> 32 -> 2) using RELU as an activation function and a dropout with a 30% probability.\n\nWe trained the whole network for 20 epochs including the pretrained part of the Densenet121, then we trained the classifier only for another 20 epochs.\n\nCan you guess the final accuracy on the test set?"
    },
    {
      "metadata": {
        "_uuid": "5fb0fc0e13b7fcdbc42963e66e3f582ff065f8ac"
      },
      "cell_type": "markdown",
      "source": "Before we start coding we have to be sure that we have imported all the modules we need. We have used Facebook's Deep Learning platform Pytorch\n\n# Pytorch \nis An open source deep learning platform that provides a seamless path from research prototyping to production deployment. [see the oficial web site](https://pytorch.org/)"
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import torchvision\nfrom torchvision import transforms,models,datasets\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch.optim as optim\nimport torch.nn as nn\nfrom collections import OrderedDict\nfrom PIL import Image\nimport seaborn as sns",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "collapsed": true,
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": false
      },
      "cell_type": "markdown",
      "source": "\n\nwe load the data now, our data is divided into 3 main folders train,validation and test set ,we will start in the first time by creating a dic of the data path so it will be easy to work with "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5afc33350dc1a3c8d36fe2e1a3bdd4e5d714b2c1"
      },
      "cell_type": "code",
      "source": "data_dir = {\n            'train': '../input/chest_xray/chest_xray/test',\n            'test': '../input/chest_xray/chest_xray/test',\n            'valid': '../input/chest_xray/chest_xray/val',\n            }",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b3a2d08f5fc002e956848021751e027ab7494dbf"
      },
      "cell_type": "markdown",
      "source": "we will use a data loaders and transformers to transform our data from images to torch tensors "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f2dcff158d6d5b068f72410e68610e6be82bbe76"
      },
      "cell_type": "code",
      "source": "batch_size = 32 # we will set the batch size to 64 \n\ndata_transforms = {\n            'train': transforms.Compose([\n                        transforms.Resize((224, 224)),\n                        transforms.CenterCrop(224),\n                        transforms.RandomHorizontalFlip(), # randomly flip and rotate\n                        transforms.RandomRotation(10),\n                        transforms.ToTensor(),\n                        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n                    ]),\n    \n            'test': transforms.Compose([\n                        transforms.Resize((224,224)),\n                        transforms.ToTensor(),\n                        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n                    ]),\n    \n            'valid': transforms.Compose([\n                        transforms.Resize((224,224)),\n                        transforms.ToTensor(),\n                        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n                    ])\n            }\n# Load the datasets with ImageFolder\n\ndata_set={\n        'train': torchvision.datasets.ImageFolder(data_dir['train'] ,data_transforms['train']),\n        'test': torchvision.datasets.ImageFolder(data_dir['test'], data_transforms['test']),\n        'valid': torchvision.datasets.ImageFolder(data_dir['valid'], data_transforms['valid']),\n         }\n\n# Using the image datasets and the trainforms, define the dataloaders\ndata_loader={\n        'train': torch.utils.data.DataLoader(data_set['train'], batch_size=batch_size,shuffle=True),\n        'test': torch.utils.data.DataLoader(data_set['test'], batch_size=batch_size,shuffle=True),\n        'valid': torch.utils.data.DataLoader(data_set['test'], batch_size=batch_size,shuffle=True),\n        }\n\n### we get the class_to_index in the data_Set but what we really need is the cat_to_names  so we will create\n_ = data_set['valid'].class_to_idx\ncat_to_name = {_[i]: i for i in list(_.keys())}\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4c8b85574bc938ba1c23c84a348255f23d825639"
      },
      "cell_type": "markdown",
      "source": "# time for some visualisation "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1c934ee70d56a0a22687ee5f22ecfe2883cf573a"
      },
      "cell_type": "code",
      "source": "def showimage(data_loader, number_images, cat_to_name):\n    dataiter = iter(data_loader)\n    images, labels = dataiter.next()\n    images = images.numpy() # convert images to numpy for display\n    # plot the images in the batch, along with the corresponding labels\n    fig = plt.figure(figsize=(number_images, 4))\n    # display 20 images\n    for idx in np.arange(number_images):\n        ax = fig.add_subplot(2, number_images/2, idx+1, xticks=[], yticks=[])\n        img = np.transpose(images[idx])\n        plt.imshow(img)\n        ax.set_title(cat_to_name[labels.tolist()[idx]])\n        \n\n#### to show some  images\nshowimage(data_loader['valid'],2,cat_to_name)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "dbe4d6af79e03301a276831a9c1eb45dbc1a3385"
      },
      "cell_type": "markdown",
      "source": "Let's now define the model that we talked about earlier in this notebook"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8971fe1e9f809ab4944b6dc87e7c80fb4129050f"
      },
      "cell_type": "code",
      "source": "model = models.densenet121(pretrained=True) # we will use a pretrained model and we are going to change only the last layer\nfor param in model.parameters():\n    param.requires_grad = True",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7946790802bb8a881c50de105455fc8c4307c7cc"
      },
      "cell_type": "code",
      "source": "model.classifier = nn.Sequential(OrderedDict([\n    ('fcl1', nn.Linear(1024,256)),\n    ('dp1', nn.Dropout(0.3)),\n    ('r1', nn.ReLU()),\n    ('fcl2', nn.Linear(256,32)),\n    ('dp2', nn.Dropout(0.3)),\n    ('r2', nn.ReLU()),\n    ('fcl3', nn.Linear(32,2)),\n    ('out', nn.LogSoftmax(dim=1)),\n]))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "442528ef1672201923075f4b9cc6b343aa34549a"
      },
      "cell_type": "markdown",
      "source": "Trainning tiime! let's first define a trainning function for reusability, then we will use it train our model.\nThis function also print some trainning information that we can use to debug any problem that may appear"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0364ca85f5543b5abe2e4a8ad860efc6778abf3c"
      },
      "cell_type": "code",
      "source": "def train_function(model, train_loader, valid_loader, criterion, optimizer, scheduler=None,\n                       train_on_gpu=False, n_epochs=30, save_file='mymodel.pth'):\n    \n    valid_loss_min = 0.218098#np.Inf\n    if train_on_gpu:\n        model.cuda()\n    for epoch in range(1, n_epochs + 1):\n        # stop training the feature CNN after epochs/2 epochs    \n        if epoch == n_epochs // 2:\n            model.load_state_dict(torch.load(save_file))\n            for param in model.features.parameters():\n                param.requires_grad = False\n            \n        train_loss = 0.0\n        valid_loss = 0.0\n        if scheduler != None:\n            scheduler.step()\n        model.train()\n        for data, target in train_loader:\n            if train_on_gpu:\n                data, target = data.cuda(), target.cuda()\n            optimizer.zero_grad()\n            output = model(data)\n            loss = criterion(output, target)\n            loss.backward()\n            optimizer.step()   \n            train_loss += loss.item() * data.size(0)\n\n        ######################    \n        # validate the model #\n        ######################\n        model.eval()\n        number_correct, number_data = 0, 0\n        for data, target in valid_loader:\n            if train_on_gpu:\n                data, target = data.cuda(), target.cuda()\n            output = model(data)\n            loss = criterion(output, target)\n            valid_loss += loss.item() * data.size(0)\n            ############# calculate the accurecy\n            _, pred = torch.max(output, 1) \n            correct_tensor = pred.eq(target.data.view_as(pred))\n            correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu \\\n                                    else np.squeeze(correct_tensor.cpu().numpy())\n            number_correct += sum(correct)\n            number_data += correct.shape[0]\n            ###################################\n        train_loss = train_loss / len(train_loader.dataset)\n        valid_loss = valid_loss / len(valid_loader.dataset)\n        accuracy = (100 * number_correct / number_data)\n        print('Epoch: {} \\n-----------------\\n \\tTraining Loss: {:.6f} \\t Validation Loss: {:.6f} \\t accuracy : {:.4f}% '.format(epoch, train_loss, valid_loss,accuracy))\n        if valid_loss <= valid_loss_min:\n            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,valid_loss))\n            torch.save(model.state_dict(), save_file)\n            valid_loss_min = valid_loss\n    model.to('cpu')\n    return torch.load(save_file)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c4b48391c957211c7b444c2e6869ca264a12d8f0"
      },
      "cell_type": "markdown",
      "source": "let's see if we can train our model on GPU , the training in the GPU is so much faster than CPU so it will be great if we can use it "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2390bf12361dee852b48f27ac68753e2ab904fba"
      },
      "cell_type": "code",
      "source": "train_on_gpu = torch.cuda.is_available()\nif train_on_gpu:\n    print('GPU is  available :)   Training on GPU ...')\nelse:\n    print('GPU is not available :(  Training on CPU ...')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ecc2b1568bb10dc9b711b15d8c30bfa917e025dd"
      },
      "cell_type": "markdown",
      "source": "Enough talking :) let's define our criterion and optimizer and start trainning!"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "70b9c48b0bc79bb230edc45b7ecf201395e0d6bc"
      },
      "cell_type": "code",
      "source": "criterion = nn.NLLLoss()\noptimizer = optim.Adadelta(model.parameters())\n\nmodel_state_dict = train_function(\n                            model,\n                            data_loader['train'],\n                            data_loader['valid'],\n                            criterion=criterion,\n                            optimizer=optimizer,\n                            scheduler=None,\n                            train_on_gpu=train_on_gpu,\n                            n_epochs=40,\n                            save_file='saved_state.pth'\n                            )\n\nmodel.load_state_dict(model_state_dict)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7c7bb0d61f3d09eeac2520039bbe6f1897d33c4f"
      },
      "cell_type": "markdown",
      "source": "After we trained our model we have to see  how it will classify data that it has never seen before, in our case it's the test set"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fe83833aee5e28f3793d96264fc7364da40fd43b"
      },
      "cell_type": "code",
      "source": "def test_function(model, test_loader, train_on_gpu, criterion,classes):\n    test_loss = 0\n    class_correct = list(0. for i in range(len(classes)))\n    class_total = list(0. for i in range(len(classes)))\n    if train_on_gpu:\n        model.cuda()\n    model.eval()\n    cat_accuracy = {}\n    if train_on_gpu:\n        model.cuda()\n    for data,target in test_loader:\n        if train_on_gpu:\n            data,target = data.cuda(),target.cuda()\n        output = model(data)\n        loss = criterion(output, target)\n        # update test loss \n        test_loss += loss.item()*data.size(0)\n        \n        _, pred = torch.max(output, 1) \n        correct_tensor = pred.eq(target.data.view_as(pred))\n        correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu \\\n                                else np.squeeze(correct_tensor.cpu().numpy())\n        for i in range(data.shape[0]):\n            label = target.data[i]\n            class_correct[label] += correct[i].item()\n            class_total[label] += 1\n    test_loss = test_loss/len(test_loader.dataset)\n    print('Test Loss: {:.6f}'.format(test_loss))\n    print('Test Accuracy (Overall): %2d%% (%2d/%2d) \\n ----------------------' % (100. * np.sum(class_correct) / np.sum(class_total),np.sum(class_correct), np.sum(class_total)))\n    for i in range(len(classes)):\n        if class_total[i] > 0:\n            print('Test Accuracy of %s : %d%% (%2d/%2d)' % (classes[i], 100 * class_correct[i] / class_total[i],np.sum(class_correct[i]), np.sum(class_total[i])))\n        else:\n            print('Test Accuracy of %s: N/A (no training examples)' % (classes[str(i+1)]))\n\n  ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0b16cf7f5020ebc823bc542385eb51f0eee09e6e"
      },
      "cell_type": "markdown",
      "source": "Let's see if you guessed the exact accuracy ..."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b0b27dca1586c9a1792fc611a043dfe400a89684"
      },
      "cell_type": "code",
      "source": "criterion = nn.NLLLoss()\ntest_function(model, data_loader['test'], train_on_gpu, criterion, cat_to_name)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "fd4001c7e333c23c095a13d905490ed3d36636f3"
      },
      "cell_type": "markdown",
      "source": "we have obtained 99% of test accuracy with just 40 epochs !! tha's great !!"
    },
    {
      "metadata": {
        "_uuid": "3b23db9faf878020157af5626aa3da31cc99142a"
      },
      "cell_type": "markdown",
      "source": "Now let's try to create some exta functions to allow our model to work with a real data (data from the web which we don't know the size ),so we create a process_image to preprocess the input which is the to be able to feed the model and see the out put and another function to show the image sow we can easily see the input  to our model and another to predict and see the probability of being noraml or sick and the last function is the function of plot that use all the other functions to predict show and plot the proba of an input"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4a5dfcdc009855d51b550976cd7f7972f514051f"
      },
      "cell_type": "code",
      "source": "def process_image(image):\n    ''' Scales, crops, and normalizes a PIL image for a PyTorch model,\n        returns an Numpy array\n    '''\n    img = Image.open(image)\n    ##########Scales \n    if img.size[0] > img.size[1]:\n        img.thumbnail((1000000, 256))\n    else:\n        img.thumbnail((256 ,1000000))\n    #######Crops: to crop the image we have to specifiy the left,Right,button and the top pixels because the crop function take a rectongle ot pixels\n    Left = (img.width - 224) / 2\n    Right = Left + 224\n    Top = (img.height - 244) / 2\n    Buttom = Top + 224\n    img = img.crop((Left, Top, Right, Buttom))\n    img = np.stack((img,)*3, axis=-1)# to repeate the the one chanel of a gray image to be RGB image \n    #img = np.repeat(image[..., np.newaxis], 3, -1)\n    #print(np.array(img).shape)\n    #normalization (divide the image by 255 so the value of the channels will be between 0 and 1 and substract the mean and divide the result by the standtared deviation)\n    img = ((np.array(img) / 255) - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])\n    img = img.transpose((2, 0, 1))\n    return img",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9c3e51ca4c79ddd790d6ec44c8215f025bbdcd9a"
      },
      "cell_type": "code",
      "source": "def imshow(image, ax=None, title=None):\n    \"\"\"Imshow for Tensor.\"\"\"\n    if ax is None:\n        fig, ax = plt.subplots()\n    if title:\n        plt.title(title)\n    # PyTorch tensors assume the color channel is the first dimension\n    # but matplotlib assumes is the third dimension\n    \n    image = image.transpose((1, 2, 0))\n    # Undo preprocessing\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    image = std * image + mean\n    \n    # Image needs to be clipped between 0 and 1 or it looks like noise when displayed\n    image = np.clip(image, 0, 1)\n    #image=np.transpose(image)\n    ax.imshow(image)\n\n    return ax",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7e667b219266bbd21b31a1a0aa1bd42856edae8a"
      },
      "cell_type": "code",
      "source": "def predict(image_path, model, topk=2):\n    ''' Predict the class (or classes) of an image using a trained deep learning model.\n    '''\n    img = process_image(image_path)\n    image_tensor = torch.from_numpy(img).type(torch.FloatTensor)\n    model_input = image_tensor.unsqueeze(0)\n    probs = torch.exp(model.forward(model_input))\n    \n    # Top probs\n    top_probs, top_labs = probs.topk(topk)\n    top_probs = top_probs.detach().numpy().tolist()[0] \n    top_labs = top_labs.detach().numpy().tolist()[0]\n    \n    # Convert indices to classes\n    #top_labels = [idx_to_class[lab] for lab in top_labs]\n    top_flowers = [cat_to_name[lab] for lab in top_labs]\n\n    return top_probs, top_flowers\n    # TODO: Implement the code to predict the class from an image file",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7d56b3295e7de9a3aac170491af77dcabe0aa36f"
      },
      "cell_type": "code",
      "source": "def plot(image_path,model,top_k=2):\n    proba, flowers = predict(image_path, model, top_k)\n    plt.figure(figsize=(6,10))\n    ax = plt.subplot(2,1,1)\n    \n    title = image_path.split('/')[5]\n    imshow(process_image(image_path), ax, title=title)\n    \n    plt.subplot(2,1,2)\n    sns.barplot(x=proba, y=flowers, color=sns.color_palette()[0]);\n    plt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "04f1bf622eb71129c4f9ba73f6cc7aade512ca04"
      },
      "cell_type": "code",
      "source": "model.to('cpu')\nplot('../input/chest_xray/chest_xray/test/NORMAL/IM-0001-0001.jpeg',model,2)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "daf8c762f7331814e8852423f08296c60ff811f5"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}